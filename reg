#MULTIPLE LINEAR REGRESSION

import numpy as np
import matplotlib as mpl
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
 
def generate_dataset(n):
    x = []
    y = []
    random_x1 = np.random.rand()
    random_x2 = np.random.rand()
    for i in range(n):
        x1 = i
        x2 = i/2 + np.random.rand()*n
        x.append([1, x1, x2])
        y.append(random_x1 * x1 + random_x2 * x2 + 1)
    return np.array(x), np.array(y)
 
x, y = generate_dataset(200)
 
mpl.rcParams['legend.fontsize'] = 12
 
fig = plt.figure()
ax = fig.gca(projection ='3d')
 
ax.scatter(x[:, 1], x[:, 2], y, label ='y', s = 5)
ax.legend()
ax.view_init(45, 0)
 
plt.show()

def mse(coef, x, y):
    return np.mean((np.dot(x, coef) - y)**2)/2

def gradients(coef, x, y):
    return np.mean(x.transpose()*(np.dot(x, coef) - y), axis = 1)

def multilinear_regression(coef, x, y, lr, b1 = 0.9, b2 = 0.999, epsilon = 1e-8):
    prev_error = 0
    m_coef = np.zeros(coef.shape)
    v_coef = np.zeros(coef.shape)
    moment_m_coef = np.zeros(coef.shape)
    moment_v_coef = np.zeros(coef.shape)
    t = 0

    while True:
        error = mse(coef, x, y)
        if abs(error - prev_error) <= epsilon:
            break
        prev_error = error
        grad = gradients(coef, x, y)
        t += 1
        m_coef = b1 * m_coef + (1-b1)*grad
        v_coef = b2 * v_coef + (1-b2)*grad**2
        moment_m_coef = m_coef / (1-b1**t)
        moment_v_coef = v_coef / (1-b2**t)

        delta = ((lr / moment_v_coef**0.5 + 1e-8) *
                (b1 * moment_m_coef + (1-b1)*grad/(1-b1**t)))

        coef = np.subtract(coef, delta)
    return coef

coef = np.array([0, 0, 0])
c = multilinear_regression(coef, x, y, 1e-1)
fig = plt.figure()
ax = fig.gca(projection ='3d')

ax.scatter(x[:, 1], x[:, 2], y, label ='y',
                s = 5, color ="dodgerblue")

ax.scatter(x[:, 1], x[:, 2], c[0] + c[1]*x[:, 1] + c[2]*x[:, 2],
                    label ='regression', s = 5, color ="orange")

ax.view_init(45, 0)
ax.legend()
plt.show()


## LINEAR REGRESSION


import numpy as np
import matplotlib.pyplot as plt

def estimate_coef(x, y):
    n = np.size(x)
    m_x = np.mean(x)
    m_y = np.mean(y)
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x

    return (b_0, b_1)

def plot_regression_line(x, y, b):
    plt.scatter(x, y, color = "m",
            marker = "o", s = 30)
    y_pred = b[0] + b[1]*x
    plt.plot(x, y_pred, color = "g")
    plt.xlabel('x')
    plt.ylabel('y')

    plt.show()

def main():
    x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])
    b = estimate_coef(x, y)
    print("Estimated coefficients:\nb_0 = {} \
        \nb_1 = {}".format(b[0], b[1]))
    plot_regression_line(x, y, b)

if __name__ == "__main__":
    main()

from sklearn.metrics import r2_score
y =[10, 20, 30]
f =[10, 20, 30]
r2 = r2_score(y, f)
print('r2 score for perfect model is', r2)


#T TEST
import math as mt
import statistics as st
import scipy.stats as ss
import operator as op

def t_test(x, y, error):
  n = len(x)
  x_mean = [st.mean(x)] * n
  x_diff = list(map(op.sub, x, x_mean))
  #x_diff = [i-j for i,j in zip(x,x_mean)]
  slope = ( n*sum(i*j for i,j in zip(x,y)) - sum(x)*sum(y) ) / ( n*sum(i*i for i in x) - (sum(x))**2 )
  SE = mt.sqrt( (sum(i**2 for i in error)/(n-2)) / mt.sqrt(sum(i**2 for i in x_diff)) )
  t_value = slope/SE
  t_table = ss.t.ppf(q=.10,df=n-2)
  return [ t_value, t_table ]

# R SQUARED FUNCTION

import statistics as st
def r2(y, error):
  sse = sum([pow(i, 2) for i in error])
  mean = st.mean(y)
  sst = sum([(i - mean)**2 for i in y])
  return 1 - (sse/sst)


#EXPONENTIAL REGRESSION

import numpy as np

def exp_reg(x, y, x_pred):
  n = len(x)
  x = list(map(np.log, x))
  y = list(map(np.log, y))
  slope = ( n*sum(i*j for i,j in zip(x,y)) - sum(x)*sum(y) ) / ( n*sum(i*i for i in x) - (sum(x))**2 )
  const = (sum(y) - slope*sum(x)) / n
  const = np.exp(const)
  return const*pow(x_pred, slope)

v = [2.27, 2.76, 3.27, 3.31, 3.7, 3.85, 4.3, 4.39, 4.42, 4.81, 4.9, 5.05, 5.21, 5.62, 5.88]
p = [2500, 365, 23700, 5491, 14000, 78200, 70700, 138000, 304500, 341948, 49375, 260200, 867023, 1340000, 1092759]

p_pred = [exp_reg(v, p, i) for i in p]


#LOGISTIC REGRESSION

import numpy as np

def log_reg(x, y, x_pred):
  n = len(x)
  slope = ( n*sum(i*j for i,j in zip(x,y)) - sum(x)*sum(y) ) / ( n*sum(i*i for i in x) - (sum(x))**2 )
  #const = (sum(y) - slope*sum(x)) / n
  return slope*np.log(x_pred)

v = [2.27, 2.76, 3.27, 3.31, 3.7, 3.85, 4.3, 4.39, 4.42, 4.81, 4.9, 5.05, 5.21, 5.62, 5.88]
p = [2500, 365, 23700, 5491, 14000, 78200, 70700, 138000, 304500, 341948, 49375, 260200, 867023, 1340000, 1092759]

v_log = list(map(np.log, v))
p_pred = [log_reg(v_log, p, i) for i in v]

